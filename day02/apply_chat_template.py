from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_name = "Qwen/Qwen2.5-0.5B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    trust_remote_code=True
)

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16,
    trust_remote_code=True
)

messages = [
    {"role": "system", "content": "ä½ æ˜¯ä¸€åè€å¿ƒçš„äººå·¥æ™ºèƒ½è€å¸ˆï¼Œæ“…é•¿ç”¨é€šä¿—çš„ä¾‹å­è§£é‡Šå¤æ‚æ¦‚å¿µã€‚"},
    {"role": "user", "content": "è¯·ç”¨å°å­¦ç”Ÿèƒ½å¬æ‡‚çš„è¯è§£é‡Šä»€ä¹ˆæ˜¯å¤§è¯­è¨€æ¨¡å‹ã€‚"}
]

# ğŸ‘‰ å…³é”®ï¼šæŠŠâ€œå¯¹è¯â€è½¬æˆæ¨¡å‹èƒ½è¯»çš„ prompt
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

inputs = tokenizer(text, return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=200
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
